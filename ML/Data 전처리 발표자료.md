# 4장 좋은 훈련 세트 만들기: 데이터 전처리

발표대상 : 도저히 이 책을 해석할 시간이 없어 발표자만 믿고 계신 여러분들 (담주부터의 제 모습)

주제
- data set 에서 누락된 값을 제거/대체하기
- ML Algorithm을 위해 Category(범주)형 데이터 변환하기
- 모델과 관련이 높은 특성 선택하기

## 4.1 문제1 : 누락된 데이터 다루기

- 실제 구현시 데이터 일부가 누락된 샘플이 많이 들어옴
  - 일반적으로 데이터 수집과정이 완벽하지 않음
  - 수집대상이 정보를 주지 않는 경우도 있음

### 4.1 대응1 : NaN, NULL 등으로 처리 (표시)
- 누락값은 보통 비워두거나 예약문자열로 채워짐 (NaN, NULL 등)
- 분석을 진행하기 전에 누락값을 처리하는 것이 필요

여기서는 데이터 샘플을 삭제하거나 다른 샘플에서 누락값을 대체하는 실용기법을 본다.
- 책은 여기를 그대로 복/붙했습니다 : https://scikit-learn.org/stable/modules/impute.html#impute

### 4.1.2 대응2 : 누락값이 있는 샘플이나 특성 제외
- 제외하는 단위
  - 샘플(row) 단위
  - feature(column) 단위
- 제외하는 기준
  - 모든 feature가 누락된 것
  - feature가 일정 개수이하인 것
  - 특정 열만 기준적용가능

결론 : 방법은 그때그때 다르다.

### 4.1.3 대응3 : 누락값 대체

- 샘플 삭제나 feature 제외가 어려울 때가 존재
  - 유용한 데이터를 너무 많이 잃는 등
- 여러가지 보간 기법을 사용하여 누락값을 추정
  - average, median, most frequent, constant value 등


## 4.2 문제2 : category(범주)형 데이터 다루기

- 순서가 있음 : 정렬가능
  - 즉, 비교 가능 : XL, L, M, S 같은 방향성이 일정한 데이터
- 순서가 없음 : 정렬불가능
  - 노랑, 초록, 빨강 등

### 4.2.3 대응1 : class label encoding

- 순서를 지정해준다 --> enumeration
  - 왜? : ML library는 보통 class label이 정수형일 것으로 기대함
- 수동 : enumerating
- 자동 : sklearn.preprocessing.LabelEncoder

dictionary mapping이라 한다.


### 4.2.4 대응2 : 순서가 없는 특성에 One-hot Encoding 적용

범주형 feature를 1,2,3 으로 변경하면 우선순위가 생긴다.
이대로 분류기에 데이터셋을 입력하는 것이 흔한 실수다.

- 대응1 : R = 1, G = 2, B = 3
- 대응2 : R = (1, 0, 0), G = (0, 1, 0), B = (0, 0, 1)

*(의문점 : 우선순위가 존재하면 학습에 어떤 영향을 끼치는가?)*


## 4.3 문제3 : 데이터셋을 훈련 세트와 테스트 세트로 나누기

- 비율 : 그때그때 달라요
  - 보통의 훈련:테스트 비율 : 6:4, 7:3, 8:2
    - 대용량의 경우 : 9:1, 99:1도 보통.
  - 학습 및 평가 후 : 테스트 세트를 버리지 말고 다시 전체 데이터셋으로 모델을 재훈련하여 성능을 올리는 것이 널리 사용됨
    - 주의사항 : 데이터셋이 작고 테스트 세트에 이상치가 있다면 성능이 나빠질 수 있음

## 4.4 문제4 : 특성 스케일 맞추기

- 전처리에서 아주 중요한 단계 (~~사실 다 중요~~)
- 특성 스케일? 특성 값의 범위. 예) 0~0.1
- 대부분의 머신 러닝과 최적화 알고리즘은 특성의 스케일이 같을 때 훨씬 성능이 좋아짐
- decision tree와 random forest는 특성 스케일 조정에 대해 걱정할 필요가 없는 알고리즘
  - 스케일에 영향을 받지 않음.

### 대응
- 두개의 특성.
  - 첫 번째 특성 스케일 : 1 ~ 10
  - 두 번째 특성 스케일 : 1 ~ 10만
- 2장의 adaline에서 제곱오차함수 : 알고리즘은 대부분 두 번째 특성에 대한 큰 오차에 맞추어 가중치를 최적화함
- 유클리디안 거리 지표를 사용한 k-최근접 이웃(K-Nearest Neighbor, KNN) : 샘플간의 거리를 계산하면 두 번째 특성 축에 좌우될 것
- 스케일이 다른 특성을 맞추는 대표적인 방법
    1. Normalization (정규화) : 대부분 특성의 스케일을 [0, 1]에 맞춤
        - Min-Max Scaling으로 구현가능 : 각 특성의 열마다 최소-최대 스케일 변환을 적용하여 샘플 x^(i)에서 새로운 값 x^(i)_norm을 계산
            - p.153 공식 사진
            
            - 스케일 범위가 정해져 있을 때 유용함
    2. Standardization (표준화)
        - 최적화 알고리즘에서 널리 사용됨
        - 특성의 평균을 0에 맞추고 표준편차를 1로 만들어 정규분포와 같은 특징을 가지도록 변환
            - p.153 공식 사진
        - 가중치를 더 쉽게 학습할 수 있도록 만듦
        - 또한, 이상치 정보가 유지되기 때문에 제한된 범위로 데이터를 조정하는 min-max scaling에 비해 이상치에 덜 민감함
            - min-max scaling은 비정상적으로 크거나 작은 값이 있을 때 다른 샘플들은 좁은 구간에 촘촘하게 모임
- p.154 표사진

## 4.5 유용한 특성 선택

### 문제 : overfitting (과적합)
- 대상 : 어떤 모델이, 테스트 데이터세트에서의 성능보다, 훈련 데이터 세트에서의 성능이 더 높은 경우, 과적합 가능성이 있음
- 대응
  - 더 많은 훈련 데이터를 모음
    - 더 모은 것이 도움이 되는지 확인 (6장)
  - 규제를 통해 복잡도를 제한
  - 파라미터 개수가 적은 간단한 모델을 선택 : 특성선택기법
  - 데이터 차원을 줄임
- 여기서는 규제, 차원축소를 사용해서 과대적합을 줄이는 방법을 본다.

### 4.5.1 대응1 : 모델 복잡도 제한을 위한 L1규제와 L2규제

다 몰라도 됩니다.
중요한건 sparse matrix를 이끌어낸다는 내용.

- - - 

3장의 L2 regularization(규제)은 개별 가중치 값을 모델 복잡도를 줄이는 방법이라고 배움
모델 복잡도를 줄이는 또 다른 방법은 L1 regularization임

p.155 수식 두개

- L2 : 가중치 제곱
- L1 : 가중치 절대값
  - 그래서 sparse feature vector (희소-특성-벡터)를 만듭니다. (이해할 필요 없습니다)
  - vector matrix에 0 이 대부분이면 행렬곱 연산 후에 feature가 소수만 남음 --> 특성을 일부만 사용
  - 실제로 관련 없는 특성이 많은 고차원 데이터셋의 경우 적용가능
  - 그래서 L1 규제는 특성 선택의 기법

### 4.5.2 대응1-1 : L2 규제의 기하학적 해석 

- L2 규제는 비용함수에 penalty term (페널티 항)을 추가함
  - 규제가 없는 비용 함수로 훈련한 모델에 비해 가중치 값을 아주 작게 만드는 효과
- L1 규제가 어떻게 희소성을 만드는가? 기하학적 해석을 보자.

p.156 그림 4-4

우리의 목표는 훈련 데이터에서 비용함수를 최소화하는 가중치 값 조합을 찾는 것

p.157 그림 4-5

몰라도 됩니다.

### 4.5.3 대응1-2 : L1 규제를 사용한 희소성

주요개념은 앞 절과 유사
L1 페널티는 가중치 절댓값의 합 -> 다이아몬드 모양의 제한범위 (L2 항은 이차식)

p.158 그림 4-6

w_1 = 0 일 때 비용 함수 등고선과 L1 다이아몬드와 만남
L1 규제의 등고선은 날카롭기 때문에 비용 함수의 포물선과 L1 다이아몬드의 경계가 만나는 최적점은 축에 가깝게 위치할 가능성이 높음
그러므로 희소성이 나타남

코드 무시

원리만 설명할 것. "bias가 0에 가까워질수록 sparse matrix 가 나오기 쉽고 이것 때문에 특징을 일부만 사용하는 것이 가능해진다"
특성(컬럼)을 줄이는 게 아닌 무시하는 것

p.161 그럼 4-7 은 무시하자그냥 쉽게 갑시다

### 4.5.4 대응2 : 순차 특성 선택 알고리즘

특성 선택을 통한 차워축소
- feature selection (특성 선택) : 원본 특성에서 일부를 선택 (이걸 설명합니다)
- feature extraction (특성 추출) : 일련의 특성에서 얻은 정보로 새로운 특성을 생성 (이건 5장)

#### Sequential Feature Selection (순차적 특성 선택)

- Greedy Search Algorithm (탐욕적 탐색 알고리즘)
  - 동작 : 초기 d차원의 특성 공간을 k < d인 k차원의 특성 부분 공간으로 축소
  - 목적
    - 주어진 문제에 가장 관련이 높은 특성 부분 집합을 자동으로 선택하는 것
    - 동시에 관계없는 특성 같은 잡음을 제거 -> 계산 효율성 높임 -> 모델의 일반화 오차를 줄여줌
  - 규제를 제공하지 않는 알고리즘을 사용할 때 유용
- 그 중 하나는 Sequential Backward Selection; SBS (순차 후진 선택)
  - 아이디어
    - 부분공간 생성 : 목표로 하는 특성 개수가 있고, 전체 특성에서 특성을 그 개수까지 순차적으로 제거합니다.
  - 동작
    - 최소화할 기준함수를 정의 : 각 단계에서 어떤 특성을 제거할지 판단
    - 기준함수에서 계산하는 값 : 어떤 특성을 제거하기 전/후의 모델 성능 차이
    - 피처를 하나씩 제거
    - 목표개수가 되면 종료
- optional : 부분공간의 특성을 보고 핵심 특성을 이해하면, 새로운 insight를 얻을 수도 있다.

p.165 그림 4-8

특성이 많이 줄어도 성능이 유지됨을 알 수 있다.
여기서 3개의 특성이 전체 특성만큼의 유용한 정보를 담고 있다는 뜻.

장점
- 특성 개수를 줄임 -> 수집해야 할 정보가 줄어듦
- 모델이 더 간단해져서 해석하기도 더 쉬워짐


### 4.6 대응3 : 랜덤 포레스트의 특성 중요도 사용

- 앙상블 기법인 랜덤포레스트를 사용하면
  - 앙상블에 참여한 모든 결정 트리에서 계산한 평균적인 불순도 감소로
    - 특성 중요도를 측정할 수 있음
- 훈련 뒤 각각의 중요도에 따라 n개의 특성에 순위를 매김
- 책에서는 임계값을 기준으로 특성을 선택함

p.169 그림 4-9

- 단점 : 두 개 이상의 특성이 매우 상관관계가 높다면 어떤 특성정보는 완전히 잡아내지 못할 수 있음
  - 다만, 중요도 값 해석보다 예측 성능이 중요하면 문제되지 않음

