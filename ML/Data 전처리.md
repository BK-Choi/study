35쪽

매일 7쪽씩

# 4장 좋은 훈련 세트 만들기: 데이터 전처리

주제
- data set 에서 누락된 값을 제거/대체하기
- ML Algorithm을 위해 범주형 데이터 변환하기
- 모델과 관련이 높은 특성 선택하기

## 4.1 누락된 데이터 다루기

실제 구현시 데이터 일부가 누락된 샘플이 많이 들어옴
데이터 수집과정이 완벽할수도 없고 수집대상이 정보를 주지 않는 경우도 있음

누락값은 보통 비워두거나 예약문자열로 채워짐 (NaN, NULL 등)
분석을 진행하기 전에 누락값을 처리하는 것이 중요 (왜? 학습이 더 잘되게 하려고? 인식용 데이터도 전처리를 해야할듯, 근데 누락값도 정보 아닌가?)

여기서는 데이터 샘플을 삭제하거나 다른 샘플에서 누락값을 대체하는 실용기법을 본다.
(https://scikit-learn.org/stable/modules/impute.html#impute 여기 내용인듯)

### 4.1.1 table 형태의 data에서 누락값 식별

음.. 그냥 python으로 null data 인식하는 것 구현만 함... 볼 거 없음 ;;;;;

### 4.1.2 누락값이 있는 샘플이나 특성 제외

누락값이 있는 경우
- 제외하는 단위
  - 샘플(row) 단위로 삭제
  - feature(column) 단위로 삭제하는 방법이 존재
- 제외하는 기준
  - 모든 feature가 누락된 것
  - feature가 일정 개수이하인 것
  - 특정 열만 기준적용가능

결론 : 그때그때 다르다.

### 4.1.3 누락값 대체

샘플 삭제나 feature 제외가 어려울 때가 있음
유용한 데이터를 너무 많이 잃기 때문

여러가지 보간 기법을 사용하여 누락값을 추정
sci-kit에서는 평균, median, most_frequent, constant 방식이 제공됨
(더 있지 않을까?)

### 4.1.4 Sci-kit Learn 추정기 API 익히기

위에서 사용한 Imputer class는 sklearn의 변환기(transformer) class임
이런 추정기(estimator)는 fit, transform method를 가짐
- fit : 훈련데이터에서 모델 파라미터를 학습
- transform : 학습한 모델 파라미터로 데이터를 변환
- 변환할 데이터 배열 : 모델학습에 사용한 데이터의 feature 개수와 같아야 함

p.142 그림 4-2
(훈련세트가 변환된다는 걸 자세히 알아보면 좋을듯)



## 4.2 범주형 데이터 다루기

수치형만 봤는데 이젠 범주형 특성을 어떻게 다루는지 봅시다.

### 4.2.1 순서가 있는 특성과 순서가 없는 특성

- 순서가 있음 : 정렬가능
  - 즉, 비교 가능
- 순서가 없음 : 정렬불가능

### 4.2.2 순서 특성 매핑

pandas의 df.map() 으로 가능허다아아아

### 4.2.3 class label encoding

순서를 지정해준다.
- 수동 : enumerating
- 자동 : sklearn.preprocessing.LabelEncoder 끝.

dictionary mapping이라 한다.


### 4.2.4 순서가 없는 특성에 One-hot Encoding 적용

범주형 feature를 1,2,3 으로 변경하면 우선순위가 생긴다.
이대로 분류기에 데이터셋을 입력하는 것이 흔한 실수다. (개인적으로 상관없다고 봄)

4.2.3 : R = 1, G = 2, B = 3
4.2.4 : R = (1, 0, 0), G = (0, 1, 0), B = (0, 0, 1)

## 4.3 데이터셋을 훈련 세트와 테스트 세트로 나누기

비율은 그때그때 달라요
- 보통의 훈련:테스트 비율 : 6:4, 7:3, 8:2
  - 대용량의 경우 : 9:1, 99:1도 보통.
- 학습 및 평가 후 : 테스트 세트를 버리지 말고 다시 전체 데이터셋으로 모델을 재훈련하여 성능을 올리는 것이 널리 사용됨
  - 주의사항 : 데이터셋이 작고 테스트 세트에 이상치가 있다면 성능이 나빠질 수 있음

## 4.4 특성 스케일 맞추기

- 전처리에서 아주 중요한 단계
- decision tree와 random forest는 특성 스케일 조정에 대해 걱정할 필요가 없는 알고리즘
  - 스케일에 영향을 받지 않음
- 대부분의 머신 러닝과 최적화 알고리즘은 특성의 스케일이 같을 때 훨씬 성능이 좋아짐

특성 스케일이 뭔데?????
### 예시
- 두개의 특성.
  - 첫 번째 특성 : 1 ~ 10의 스케일
  - 두 번째 특성 : 1 ~ 10만의 스케일
- 2장의 adaline에서 제곱오차함수 : 알고리즘은 대부분 두 번째 특성에 대한 큰 오차에 맞추어 가중치를 최적화함
- 유클리디안 거리 지표를 사용한 k-최근접 이웃(K-Nearest Neighbor, KNN) : 샘플간의 거리를 계산하면 두 번째 특성 축에 좌우될 것
- 스케일이 다른 특성을 맞추는 대표적인 방법
    1. Normalization (정규화) : 대부분 특성의 스케일을 [0, 1]에 맞춤
        - Min-Max Scaling으로 구현가능 : 각 특성의 열마다 최소-최대 스케일 변환을 적용하여 샘플 x^(i)에서 새로운 값 x^(i)_norm을 계산
            - p.153 공식 사진
            - 스케일 범위가 정해져 있을 때 유용함
    2. Standardization (표준화)
        - 최적화 알고리즘에서 널리 사용됨
        - 특성의 평균을 0에 맞추고 표준편차를 1로 만들어 정규분포와 같은 특징을 가지도록 변환
            - p.153 공식 사진
        - 가중치를 더 쉽게 학습할 수 있도록 만듦
        - 또한, 이상치 정보가 유지되기 때문에 제한된 범위로 데이터를 조정하는 min-max scaling에 비해 이상치에 덜 민감함
            - min-max scaling은 비정상적으로 크거나 작은 값이 있을 때 다른 샘플들은 좁은 구간에 촘촘하게 모임
- p.154 표사진
- fit method를 훈련 세트에만 딱 한 번 적용한 것이 포인트 : 학습한 파라미터로 테스트 세트와 새로운 데이터 포인트를 모두 변환함


## 4.5 유용한 특성 선택



## 4.6 랜덤 포레스트의 특성 중요도 사용
## 4.7 요약

