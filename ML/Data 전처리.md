35쪽

매일 7쪽씩

# 4장 좋은 훈련 세트 만들기: 데이터 전처리

주제
- data set 에서 누락된 값을 제거/대체하기
- ML Algorithm을 위해 범주형 데이터 변환하기
- 모델과 관련이 높은 특성 선택하기

## 4.1 누락된 데이터 다루기

실제 구현시 데이터 일부가 누락된 샘플이 많이 들어옴
데이터 수집과정이 완벽할수도 없고 수집대상이 정보를 주지 않는 경우도 있음

누락값은 보통 비워두거나 예약문자열로 채워짐 (NaN, NULL 등)
분석을 진행하기 전에 누락값을 처리하는 것이 중요 (왜? 학습이 더 잘되게 하려고? 인식용 데이터도 전처리를 해야할듯, 근데 누락값도 정보 아닌가?)

여기서는 데이터 샘플을 삭제하거나 다른 샘플에서 누락값을 대체하는 실용기법을 본다.
(https://scikit-learn.org/stable/modules/impute.html#impute 여기 내용인듯)

### 4.1.1 table 형태의 data에서 누락값 식별

음.. 그냥 python으로 null data 인식하는 것 구현만 함... 볼 거 없음 ;;;;;

### 4.1.2 누락값이 있는 샘플이나 특성 제외

누락값이 있는 경우
- 제외하는 단위
  - 샘플(row) 단위로 삭제
  - feature(column) 단위로 삭제하는 방법이 존재
- 제외하는 기준
  - 모든 feature가 누락된 것
  - feature가 일정 개수이하인 것
  - 특정 열만 기준적용가능

결론 : 그때그때 다르다.

### 4.1.3 누락값 대체

샘플 삭제나 feature 제외가 어려울 때가 있음
유용한 데이터를 너무 많이 잃기 때문

여러가지 보간 기법을 사용하여 누락값을 추정
sci-kit에서는 평균, median, most_frequent, constant 방식이 제공됨
(더 있지 않을까?)

### 4.1.4 Sci-kit Learn 추정기 API 익히기

위에서 사용한 Imputer class는 sklearn의 변환기(transformer) class임
이런 추정기(estimator)는 fit, transform method를 가짐
- fit : 훈련데이터에서 모델 파라미터를 학습
- transform : 학습한 모델 파라미터로 데이터를 변환
- 변환할 데이터 배열 : 모델학습에 사용한 데이터의 feature 개수와 같아야 함

p.142 그림 4-2
(훈련세트가 변환된다는 걸 자세히 알아보면 좋을듯)



## 4.2 범주형 데이터 다루기

수치형만 봤는데 이젠 범주형 특성을 어떻게 다루는지 봅시다.

### 4.2.1 순서가 있는 특성과 순서가 없는 특성

- 순서가 있음 : 정렬가능
  - 즉, 비교 가능
- 순서가 없음 : 정렬불가능

### 4.2.2 순서 특성 매핑

pandas의 df.map() 으로 가능허다아아아

### 4.2.3 class label encoding

순서를 지정해준다.
- 수동 : enumerating
- 자동 : sklearn.preprocessing.LabelEncoder 끝.

dictionary mapping이라 한다.


### 4.2.4 순서가 없는 특성에 One-hot Encoding 적용

범주형 feature를 1,2,3 으로 변경하면 우선순위가 생긴다.
이대로 분류기에 데이터셋을 입력하는 것이 흔한 실수다. (개인적으로 상관없다고 봄)

4.2.3 : R = 1, G = 2, B = 3
4.2.4 : R = (1, 0, 0), G = (0, 1, 0), B = (0, 0, 1)

## 4.3 데이터셋을 훈련 세트와 테스트 세트로 나누기

비율은 그때그때 달라요
- 보통의 훈련:테스트 비율 : 6:4, 7:3, 8:2
  - 대용량의 경우 : 9:1, 99:1도 보통.
- 학습 및 평가 후 : 테스트 세트를 버리지 말고 다시 전체 데이터셋으로 모델을 재훈련하여 성능을 올리는 것이 널리 사용됨
  - 주의사항 : 데이터셋이 작고 테스트 세트에 이상치가 있다면 성능이 나빠질 수 있음

## 4.4 특성 스케일 맞추기

- 전처리에서 아주 중요한 단계
- decision tree와 random forest는 특성 스케일 조정에 대해 걱정할 필요가 없는 알고리즘
  - 스케일에 영향을 받지 않음
- 대부분의 머신 러닝과 최적화 알고리즘은 특성의 스케일이 같을 때 훨씬 성능이 좋아짐

특성 스케일이 뭔데?????
### 예시
- 두개의 특성.
  - 첫 번째 특성 : 1 ~ 10의 스케일
  - 두 번째 특성 : 1 ~ 10만의 스케일
- 2장의 adaline에서 제곱오차함수 : 알고리즘은 대부분 두 번째 특성에 대한 큰 오차에 맞추어 가중치를 최적화함
- 유클리디안 거리 지표를 사용한 k-최근접 이웃(K-Nearest Neighbor, KNN) : 샘플간의 거리를 계산하면 두 번째 특성 축에 좌우될 것
- 스케일이 다른 특성을 맞추는 대표적인 방법
    1. Normalization (정규화) : 대부분 특성의 스케일을 [0, 1]에 맞춤
        - Min-Max Scaling으로 구현가능 : 각 특성의 열마다 최소-최대 스케일 변환을 적용하여 샘플 x^(i)에서 새로운 값 x^(i)_norm을 계산
            - p.153 공식 사진
            - 스케일 범위가 정해져 있을 때 유용함
    2. Standardization (표준화)
        - 최적화 알고리즘에서 널리 사용됨
        - 특성의 평균을 0에 맞추고 표준편차를 1로 만들어 정규분포와 같은 특징을 가지도록 변환
            - p.153 공식 사진
        - 가중치를 더 쉽게 학습할 수 있도록 만듦
        - 또한, 이상치 정보가 유지되기 때문에 제한된 범위로 데이터를 조정하는 min-max scaling에 비해 이상치에 덜 민감함
            - min-max scaling은 비정상적으로 크거나 작은 값이 있을 때 다른 샘플들은 좁은 구간에 촘촘하게 모임
- p.154 표사진
- fit method를 훈련 세트에만 딱 한 번 적용한 것이 포인트 : 학습한 파라미터로 테스트 세트와 새로운 데이터 포인트를 모두 변환함


## 4.5 유용한 특성 선택

### overfitting (과적합)
- 증상 :  훈련 데이터의 성능이 테스트 데이터세트보다 높음
- 대응
  - 더 많은 훈련 데이터를 모음
    - 더 모은 것이 도움이 되는지 확인 (6장)
  - 규제를 통해 복잡도를 제한
  - 파라미터 개수가 적은 간단한 모델을 선택 : 특성선택기법
  - 데이터 차원을 줄임
- 여기서는 규제, 차원축소를 사용해서 과대적합을 줄이는 방법을 본다.

### 4.5.1 모델 복잡도 제한을 위한 L1규제와 L2규제

3장의 L2 regularization(규제)은 개별 가중치 값을 모델 복잡도를 줄이는 방법이라고 배움
모델 복잡도를 줄이는 또 다른 방법은 L1 regularization임

p.155 수식 두개

- L2 : 가중치 제곱
- L1 : 가중치 절대값
  - 그래서 sparse feature vector (희소-특성-벡터)를 만듭니다. (이해할 필요 없습니다)
  - vector matrix에 0 이 대부분이면 행렬곱 연산 후에 feature가 소수만 남음 --> 특성을 일부만 사용
  - 실제로 관련 없는 특성이 많은 고차원 데이터셋의 경우 적용가능
  - 그래서 L1 규제는 특성 선택의 기법

### 4.5.2 L2 규제의 기하학적 해석

- L2 규제는 비용함수에 penalty term (페널티 항)을 추가함
  - 규제가 없는 비용 함수로 훈련한 모델에 비해 가중치 값을 아주 작게 만드는 효과
- L1 규제가 어떻게 희소성을 만드는가? 기하학적 해석을 보자.

p.156 그림 4-4

우리의 목표는 훈련 데이터에서 비용함수를 최소화하는 가중치 값 조합을 찾는 것

p.157 그림 4-5

모델을 학습할 만한 충분한 훈련 데이터가 없을 때 편향을 추가하여 모델을 간단하게 만듦으로써 분산을 줄이는 것으로 "해석" (정확히 모른다?)

### 4.5.3 L1 규제를 사용한 희소성

주요개념은 앞 절과 유사
L1 페널티는 가중치 절댓값의 합 -> 다이아몬드 모양의 제한범위 (L2 항은 이차식)

p.158 그림 4-6

w_1 = 0 일 때 비용 함수 등고선과 L1 다이아몬드와 만남
L1 규제의 등고선은 날카롭기 때문에 비용 함수의 포물선과 L1 다이아몬드의 경계가 만나는 최적점은 축에 가깝게 위치할 가능성이 높음
그러므로 희소성이 나타남

코드 무시

원리만 설명할 것. "bias가 0에 가까워질수록? sparse matrix 가 나오기 쉽고 이것 때문에 특징을 일부만 사용하는 것이 가능해진다"

p.161 그럼 4-7 은 무시하자그냥 쉽게 갑시다



## 4.6 랜덤 포레스트의 특성 중요도 사용
## 4.7 요약

